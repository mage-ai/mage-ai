project_type: main

variables_dir: ~/.mage_data
remote_variables_dir: s3://bucket/path_prefix

variables_retention_period: '90d'

emr_config:
  # You can customize the EMR cluster instance size with the two parameters
  master_instance_type: 'r5.4xlarge'
  slave_instance_type: 'r5.4xlarge'

  # Configure security groups for EMR cluster instances.
  # The default managed security groups are ElasticMapReduce-master and ElasticMapReduce-slave
  # master_security_group: 'sg-xxxxxxxxxxxx'
  # slave_security_group: 'sg-yyyyyyyyyyyy'

  # If you want to ssh tunnel into EMR cluster, ec2_key_name must be configured.
  # You can create a key pair in page https://console.aws.amazon.com/ec2#KeyPairs and download the key file.
  # ec2_key_name: '[ec2_key_pair_name]'

hdinsight_config:
  # You can customize the HDInsight cluster with the following parameters
  # Tenant ID for your Azure Subscription
  tenant_id: '00000000-0000-0000-0000-000000000000'
  # Your Service Principal App Client ID
  client_id: '00000000-0000-0000-0000-000000000000'
  # Your Service Principal Client Secret
  client_secret: ''
  # Azure Subscription ID
  subscription_id: '00000000-0000-0000-0000-000000000000'
  # The name of the resource group that contains the cluster.
  resource_group_name: ''
  # The name for the cluster you are creating. The name must be unique, 59 characters or less,
  # and can contain letters, numbers, and hyphens (but the first and last character must be
  # a letter or number).
  # Required by all the samples.
  cluster_name: 'mage-data-prep'
  # Choose a region. i.e. "East US 2".
  location: ''
  # Choose a cluster login username. The username must be at least two characters
  # in length and can only consist of digits, upper or lowercase letters,
  # and/or the following special characters: ! # $ % & ' ( ) - ^ _ ` { } ~
  cluster_login_user_name: 'admin'
  # Choose a Secure Shell (SSH) user username. The SSH username must be at least two characters
  # in length and can only consist of digits, upper or lowercase letters,
  # and/or the following special characters: % & ' - ^ _ ` {} ~
  ssh_user_name: 'sshuser'
  # Choose a cluster admin password. The password must be at least 10 characters in length
  # and must contain at least one digit, one uppercase and one lower case letter,
  # one non-alphanumeric character (except characters ' " ` \).
  password: ''
  # The name of blob storage account
  storage_account_name: ''
  # Blob storage account key
  storage_account_key: ''
  # Blob Storage endpoint suffix.
  blob_endpoint_suffix: '.blob.core.windows.net'
  # Blob storage account container name
  container_name: 'mage-storage-container'
  # HDInsight cluster version
  cluster_version: '3.6'
  # The type of HDInsight cluster you want to create.
  kind: 'Spark'
  # The size of the VMs in the cluster for headnodes. See available VM sizes at
  # https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-provision-linux-clusters#configure-cluster-size-and-type
  headnode_vm_size: 'Large'
  # The size of the VMs in the cluster for workernodes
  workernode_vm_size: 'Large'

notification_config:
  alert_on:
    - trigger_failure
    - trigger_passed_sla
  slack_config:
    webhook_url: "{{ env_var('MAGE_SLACK_WEBHOOK_URL') }}"
  teams_config:
    webhook_url: "{{ env_var('MAGE_TEAMS_WEBHOOK_URL') }}"

retry_config:
  # Number of retry times
  retries: 0
  # Initial delay before retry. If exponential_backoff is true,
  # the delay time is multiplied by 2 for the next retry
  delay: 5
  # Maximum time between the first attempt and the last retry
  max_delay: 60
  # Whether to use exponential backoff retry
  exponential_backoff: true

spark_config:
  # Application name
  app_name: 'my spark app'
  # Master URL to connect to
  # e.g., spark_master: 'spark://host:port', or spark_master: 'yarn'
  spark_master: 'local'
  # Executor environment variables
  # e.g., executor_env: {'PYTHONPATH': '/home/path'}
  executor_env: {}
  # Jar files to be uploaded to the cluster and added to the classpath
  # e.g., spark_jars: ['/home/path/example1.jar']
  spark_jars: []
  # Path where Spark is installed on worker nodes,
  # e.g. spark_home: '/usr/lib/spark'
  spark_home: null
  # List of key-value pairs to be set in SparkConf
  # e.g., others: {'spark.executor.memory': '4g', 'spark.executor.cores': '2'}
  others: {}
