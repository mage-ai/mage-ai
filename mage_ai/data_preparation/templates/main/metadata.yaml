project_type: main

variables_dir: ~/.mage_data
# remote_variables_dir: s3://bucket/path_prefix

variables_retention_period: '90d'

emr_config:
  # You can customize the EMR cluster instance size with the two parameters
  master_instance_type: 'r5.4xlarge'
  slave_instance_type: 'r5.4xlarge'

  # Configure security groups for EMR cluster instances.
  # The default managed security groups are ElasticMapReduce-master and ElasticMapReduce-slave
  # master_security_group: 'sg-xxxxxxxxxxxx'
  # slave_security_group: 'sg-yyyyyyyyyyyy'

  # If you want to ssh tunnel into EMR cluster, ec2_key_name must be configured.
  # You can create a key pair in page https://console.aws.amazon.com/ec2#KeyPairs and download the key file.
  # ec2_key_name: '[ec2_key_pair_name]'

notification_config:
  alert_on:
    - trigger_failure
    - trigger_passed_sla
  slack_config:
    webhook_url: "{{ env_var('MAGE_SLACK_WEBHOOK_URL') }}"
  teams_config:
    webhook_url: "{{ env_var('MAGE_TEAMS_WEBHOOK_URL') }}"

retry_config:
  # Number of retry times
  retries: 0
  # Initial delay before retry. If exponential_backoff is true,
  # the delay time is multiplied by 2 for the next retry
  delay: 5
  # Maximum time between the first attempt and the last retry
  max_delay: 60
  # Whether to use exponential backoff retry
  exponential_backoff: true

spark_config:
  # Application name
  app_name: 'my spark app'
  # Master URL to connect to
  # e.g., spark_master: 'spark://host:port', or spark_master: 'yarn'
  spark_master: 'local'
  # Executor environment variables
  # e.g., executor_env: {'PYTHONPATH': '/home/path'}
  executor_env: {}
  # Jar files to be uploaded to the cluster and added to the classpath
  # e.g., spark_jars: ['/home/path/example1.jar']
  spark_jars: []
  # Path where Spark is installed on worker nodes,
  # e.g. spark_home: '/usr/lib/spark'
  spark_home: null
  # List of key-value pairs to be set in SparkConf
  # e.g., others: {'spark.executor.memory': '4g', 'spark.executor.cores': '2'}
  others: {}
