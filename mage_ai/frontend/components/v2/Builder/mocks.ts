// @ts-nocheck
import { PipelineType, PipelineTypeEnum } from '@interfaces/PipelineType';
import { BlockTypeEnum } from '@interfaces/BlockType';

const PIPELINE_DYNAMIC: PipelineType = {
  cache_block_output_in_memory: false,
  concurrency_config: {
    block_run_limit: 40,
  },
  created_at: '2024-05-26 07:48:27.696425+00:00',
  data_integration: null,
  description: null,
  executor_config: {},
  executor_count: 1,
  executor_type: null,
  name: 'dynamic',
  notification_config: {},
  remote_variables_dir: null,
  retry_config: {},
  run_pipeline_in_one_process: false,
  settings: {
    triggers: null,
  },
  tags: [],
  type: 'python',
  uuid: 'dynamic',
  variables_dir: '/root/.mage_data/memory_upgrade_v2',
  spark_config: {},
  blocks: [
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        dynamic: {
          modes: [
            {
              type: 'stream',
            },
          ],
          parent: true,
        },
        file_path: 'memory_upgrade_v2/data_loaders/dynamic1.py',
        file_source: {
          path: 'memory_upgrade_v2/data_loaders/dynamic1.py',
        },
      },
      downstream_blocks: ['child_2x', 'child_1x_childspawn_1x_reduce'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'dynamic1',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'data_loader',
      upstream_blocks: [],
      uuid: 'dynamic1',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "@data_loader\ndef load_data1(*args, **kwargs):\n    arr = [i + 10 for i in range(0, 2)]\n    return [\n        arr,\n        [dict(block_uuid=f'child_{i}') for i in arr],\n    ]",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356602,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        dynamic: {
          modes: [
            {
              type: 'stream',
            },
          ],
          parent: false,
        },
        file_path: 'memory_upgrade_v2/data_loaders/dynamic2.py',
        file_source: {
          path: 'memory_upgrade_v2/data_loaders/dynamic2.py',
        },
      },
      downstream_blocks: ['child_2x', 'child_1x_spawn_1x'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'dynamic2',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'data_loader',
      upstream_blocks: [],
      uuid: 'dynamic2',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "@data_loader\ndef load_data2(*args, **kwargs):\n    arr = [i + 20 for i in range(0, 2)]\n    return [\n        arr,\n        [dict(block_uuid=f'child_{i}') for i in arr],\n    ]",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356658,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_path: 'memory_upgrade_v2/transformers/child_2x.py',
        file_source: {
          path: 'memory_upgrade_v2/transformers/child_2x.py',
        },
      },
      downstream_blocks: ['child_1x', 'dynamic_spawn_2x'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'child 2x',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'transformer',
      upstream_blocks: ['dynamic1', 'dynamic2'],
      uuid: 'child_2x',
      callback_blocks: [],
      conditional_blocks: [],
      content: '',
      outputs: [],
      error: {
        error: 'No such file or directory',
        message:
          'You may have moved it or changed its filename. Delete the current block to remove it from the pipeline or write code and save the pipeline to create a new file at /home/src/default_repo/mlops/mlops/memory_upgrade_v2/transformers/child_2x.py.',
      },
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.3567,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        dynamic: true,
        file_path: 'memory_upgrade_v2/data_exporters/dynamic_spawn_2x.py',
        file_source: {
          path: 'memory_upgrade_v2/data_exporters/dynamic_spawn_2x.py',
        },
      },
      downstream_blocks: ['child_1x_spawn_1x', 'replica'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'dynamic spawn 2x',
      language: 'python',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'data_exporter',
      upstream_blocks: ['child_2x', 'child_1x'],
      uuid: 'dynamic_spawn_2x',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "@data_exporter\ndef transform_2_1(arr, number, **kwargs):\n    arr = [arr, kwargs.get('upstream_data', 0), number]\n    return [\n        arr,\n    ]",
      outputs: [],
      metadata: {},
      tags: ['dynamic'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.35674,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'teal',
      configuration: {
        file_path: 'memory_upgrade_v2/custom/child_1x.py',
        file_source: {
          path: 'memory_upgrade_v2/custom/child_1x.py',
        },
      },
      downstream_blocks: ['child_1x_spawn_1x', 'dynamic_spawn_2x'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'child 1x',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'custom',
      upstream_blocks: ['child_2x'],
      uuid: 'child_1x',
      callback_blocks: [],
      conditional_blocks: [],
      content: '@custom\ndef transform(data, *args, **kwargs):\n    return data',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356759,
        },
      ],
    },
    {
      all_upstream_blocks_executed: false,
      color: null,
      configuration: {
        file_path: 'memory_upgrade_v2/transformers/child_1x_spawn_1x.py',
        file_source: {
          path: 'memory_upgrade_v2/transformers/child_1x_spawn_1x.py',
        },
        reduce_output: true,
      },
      downstream_blocks: ['child_1x_childspawn_1x_reduce', 'replica'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'child 1x spawn 1x',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'transformer',
      upstream_blocks: ['dynamic2', 'dynamic_spawn_2x', 'child_1x'],
      uuid: 'child_1x_spawn_1x',
      callback_blocks: [],
      conditional_blocks: [],
      content: '',
      outputs: [
        {
          sample_data: {
            columns: ['output'],
            rows: [[{}]],
          },
          shape: [1, 1],
          type: 'table',
          variable_uuid: 'output_1',
          multi_output: true,
        },
      ],
      error: {
        error: 'No such file or directory',
        message:
          'You may have moved it or changed its filename. Delete the current block to remove it from the pipeline or write code and save the pipeline to create a new file at /home/src/default_repo/mlops/mlops/memory_upgrade_v2/transformers/child_1x_spawn_1x.py.',
      },
      metadata: {},
      tags: ['dynamic_child', 'reduce_output'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356797,
        },
      ],
    },
    {
      all_upstream_blocks_executed: false,
      color: null,
      configuration: {
        dynamic: true,
        file_path: 'memory_upgrade_v2/data_exporters/child_1x_childspawn_1x_reduce.py',
        file_source: {
          path: 'memory_upgrade_v2/data_exporters/child_1x_childspawn_1x_reduce.py',
        },
      },
      downstream_blocks: ['child_1x_childspawn_1x_reduce'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'replica',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'data_exporter',
      upstream_blocks: ['dynamic_spawn_2x', 'child_1x_spawn_1x'],
      uuid: 'replica',
      callback_blocks: [],
      conditional_blocks: [],
      replicated_block: 'child_1x_childspawn_1x_reduce',
      content:
        '@data_exporter\ndef export_data(*args, **kwargs):\n    output = []\n    for input_data in args:\n        if isinstance(input_data, list):\n            for data in input_data:\n                output.append(data)\n        else:\n            output.append(input_data)\n    \n    return output',
      outputs: [
        {
          sample_data: {
            columns: ['dynamic child blocks'],
            rows: [[{}]],
          },
          shape: [1, 1],
          type: 'table',
          variable_uuid: 'output_1',
          multi_output: true,
        },
      ],
      metadata: {},
      tags: ['dynamic', 'dynamic_child', 'replica'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356834,
        },
      ],
    },
    {
      all_upstream_blocks_executed: false,
      color: null,
      configuration: {
        file_path: 'memory_upgrade_v2/data_exporters/child_1x_childspawn_1x_reduce.py',
        file_source: {
          path: 'memory_upgrade_v2/data_exporters/child_1x_childspawn_1x_reduce.py',
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'child 1x childspawn 1x reduce',
      language: 'python',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'data_exporter',
      upstream_blocks: ['dynamic1', 'child_1x_spawn_1x', 'replica'],
      uuid: 'child_1x_childspawn_1x_reduce',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '@data_exporter\ndef export_data(*args, **kwargs):\n    output = []\n    for input_data in args:\n        if isinstance(input_data, list):\n            for data in input_data:\n                output.append(data)\n        else:\n            output.append(input_data)\n    \n    return output',
      outputs: [
        {
          sample_data: {
            columns: ['output'],
            rows: [[{}]],
          },
          shape: [1, 1],
          type: 'table',
          variable_uuid: 'output_1',
          multi_output: true,
        },
      ],
      metadata: {},
      tags: ['dynamic_child'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-26 07:48:27.696425+00:00',
            description: null,
            name: 'dynamic',
            tags: [],
            type: 'python',
            uuid: 'dynamic',
            repo_path: 'memory_upgrade_v2',
          },
          updated_at: 1718506200.356834,
        },
      ],
    },
  ],
  callbacks: [],
  conditionals: [],
  widgets: [],
  extensions: {},
  updated_at: '2024-05-28T15:39:02.396763+00:00',
} as PipelineType;

const PIPELINE_DEPLOY = {
  cache_block_output_in_memory: false,
  concurrency_config: {},
  created_at: '2024-05-16 12:07:35.124087+00:00',
  data_integration: null,
  description: null,
  executor_config: {},
  executor_count: 1,
  executor_type: null,
  name: 'Deploying to production',
  notification_config: {},
  remote_variables_dir: null,
  retry_config: {},
  run_pipeline_in_one_process: false,
  settings: {
    triggers: null,
  },
  tags: [],
  type: 'python',
  uuid: 'deploying_to_production',
  variables_dir: '/root/.mage_data/unit_3_observability',
  variables: {
    destroy: 1,
    password: 'password',
    prevent_destroy_ecr: false,
  },
  spark_config: {},
  blocks: [
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Running operations in production',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'running_operations_in_production',
      callback_blocks: [],
      conditional_blocks: [],
      content: '# 3.5 Deploying: Running operations in production',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.988163,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Create IAM policy to deploy Mage',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'create_iam_policy_to_deploy_mage',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '## Setup AWS permissions for deploying\n\nRun the next block to set all of this up automtically.\n\n### Create IAM policy to deploy Mage\n\n1. Add these\n   [permissions](https://docs.mage.ai/production/deploying-to-cloud/aws/terraform-apply-policy)\n   using the JSON editor.\n\n1. Policy name: **TerraformApplyDeployMage**\n\n> In order to perform the above action, you need the following permissions:\n\n```json\niam:CreatePolicy\n```\n\n```sh\naws iam create-policy --policy-name TerraformApplyDeployMage --policy-document \\\n  "$(curl -s https://raw.githubusercontent.com/mage-ai/mage-ai-terraform-templates/master/aws/policies/TerraformApplyDeployMage.json)"\n```\n\n<br />\n\n---\n\n<br />\n\n### Create AWS policies to delete resources\n\n1. Add these\n   [permissions](https://docs.mage.ai/production/deploying-to-cloud/aws/terraform-destroy-policy)\n   using the JSON editor.\n\n1. Policy name: **TerraformDestroyDeleteResources**\n\n> In order to perform the above action, you need the following permissions:\n\n```json\niam:CreatePolicy\n```\n\n```sh\naws iam create-policy --policy-name TerraformDestroyDeleteResources --policy-document \\\n  "$(curl -s https://raw.githubusercontent.com/mage-ai/mage-ai-terraform-templates/master/aws/policies/TerraformDestroyDeleteResources.json)"\n```\n\n<br />\n\n---\n\n<br />\n\n### Create IAM user\n\n1. User name: **MageDeployer**\n\n1. Attach policies directly:\n\n- **TerraformApplyDeployMage**\n- **TerraformDestroyDeleteResources**\n\n1. After creating the user, create an access key and secret key.\n\n1. Use case: **Command Line Interface (CLI)**\n\n##### Create IAM User\n\n**Creating the IAM User (MageDeployer):**\n\n> In order to perform the above action, you need the following permissions:\n\n```json\niam:CreateUser\n```\n\n```sh\naws iam create-user --user-name MageDeployer\n```\n\n##### Attach Policies to User\n\n> In order to perform the above action, you need the following permissions:\n\n```json\niam:ListPolicies\n```\n\n**Attach TerraformApplyDeployMage Policy:**\n\n_First, find the Policy ARN by listing the policies.\n\\_Then, attach the policy to the user using the ARN obtained:_\n\n```sh\naws iam attach-user-policy \\\n  --policy-arn $(aws iam list-policies \\\n  --query "Policies[?PolicyName==\\`TerraformApplyDeployMage\\`].Arn" \\\n  --output text) \\\n  --user-name MageDeployer\n```\n\n**Attach TerraformDestroyDeleteResources Policy:**\n\n_Find the Policy ARN. \\_Attach the policy to the user:_\n\n```sh\naws iam attach-user-policy \\\n  --policy-arn $(aws iam list-policies \\\n  --query "Policies[?PolicyName==\\`TerraformDestroyDeleteResources\\`].Arn" \\\n  --output text) \\\n  --user-name MageDeployer\n```\n\n##### Create Access Key for the User\n\n```sh\naws iam create-access-key \\\n  --user-name MageDeployer \\\n  --output json | jq -r \'"[mage-deployer]\\naws_access_key_id = \\(.AccessKey.AccessKeyId)\\naws_secret_access_key = \\(.AccessKey.SecretAccessKey)"\' >> ~/.aws/credentials\nexport AWS_PROFILE="mage-deployer"\n```\n',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.988737,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'yellow',
      configuration: {
        file_path: 'unit_5_deploying/custom/permissions.py',
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['infrastructure_setup'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Permissions',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'custom',
      upstream_blocks: [],
      uuid: 'permissions',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from mlops.utils.deploy.aws import (\n    IAM_USER_NAME,\n    POLICY_NAME_TERRAFORM_APPLY_DEPLOY_MAGE,\n    POLICY_NAME_TERRAFORM_DESTROY_DELETE_RESOURCES,\n    TERRAFORM_APPLY_URL,\n    TERRAFORM_DESTROY_URL,\n    attach_policy_to_user,\n    create_access_key_for_user,\n    create_policy,\n    create_user,\n    reset,\n    save_credentials_to_file,\n)\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef setup(*args, **kwargs):\n    reset(IAM_USER_NAME)\n\n    # Create IAM Policies\n    terraform_apply_policy_arn = create_policy(\n        POLICY_NAME_TERRAFORM_APPLY_DEPLOY_MAGE, TERRAFORM_APPLY_URL\n    )\n    terraform_destroy_policy_arn = create_policy(\n        POLICY_NAME_TERRAFORM_DESTROY_DELETE_RESOURCES, TERRAFORM_DESTROY_URL\n    )\n\n    # Create the user MageDeployer\n    create_user(IAM_USER_NAME)\n\n    # Attach policies to the user MageDeployer\n    attach_policy_to_user(IAM_USER_NAME, terraform_apply_policy_arn)\n    attach_policy_to_user(IAM_USER_NAME, terraform_destroy_policy_arn)\n\n    # Create access key\n    access_key, secret_key = create_access_key_for_user(IAM_USER_NAME)\n    save_credentials_to_file(IAM_USER_NAME, access_key, secret_key)\n",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Terraform setup docs',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'terraform_setup_docs',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '## Terraform setup\n\n> If you cloned Mage’s [MLOps repository](https://github.com/mage-ai/mlops), \n> skip this section and go to the **Customize Terraform configurations** section.\n\n\nInstall and setup Terraform on your local machine.\nFollow the guide here or the detailed guide in the \n[Mage arcane library aka developer documentation](https://docs.mage.ai/production/deploying-to-cloud/using-terraform).\n\n<br />\n\n---\n\n<br />\n\n\n### Customize Terraform configurations\n\nOpen the file `mage-ops/mlops/terraform/aws/variables.tf` and update the following variables:\n\n```hcl\nvariable "docker_image" {\n  description = "Docker image url used in ECS task."\n  default     = "mageai/mageai:alpha"\n}\n\nvariable "app_name" {\n  type        = string\n  description = "Application Name"\n  default     = "mlops"\n}\n\nvariable "aws_region" {\n  type        = string\n  description = "AWS Region"\n  default     = "us-west-2"\n}\n\nvariable "availability_zones" {\n  description = "List of availability zones"\n  default     = ["us-west-2a", "us-west-2b"]\n}\n```\n\nOpen the file `mage-ops/mlops/terraform/aws/env_vars.json` and update 1 variable\nif your project name isn’t `mlops`:\n\n```json\n[\n  {\n    "name": "MAGE_PRESENTERS_DIRECTORY",\n    "value": "[change to your project name]/presenters"\n  }\n]\n```\n\n<br />\n\n---\n\n<br />\n\n#### Run the following commands to deploy the application to AWS:\n\nRun the command after changing directories into: `mage-ops/mlops/terraform/aws`\n\n```bash\nterraform init\nterraform apply\n```\n\nHere are detailed instructions on how to\n[deploy Mage to AWS using Terraform](https://docs.mage.ai/production/deploying-to-cloud/aws/setup).',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.98895,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'blue',
      configuration: {
        file_path: 'unit_5_deploying/custom/infrastructure_setup.py',
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['teardown_deployed_resources'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Infrastructure setup',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'custom',
      upstream_blocks: ['permissions'],
      uuid: 'infrastructure_setup',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "import os\n\nfrom mlops.utils.deploy.terraform.env_vars import set_environment_variables\nfrom mlops.utils.deploy.terraform.setup import download_terraform_configurations, setup_configurations\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef setup(*args, **kwargs):\n    \"\"\"\n    Downloads the base configurations for Terraform maintained and provided by Mage\n    https://github.com/mage-ai/mage-ai-terraform-templates\n    \"\"\"\n    download_terraform_configurations()\n\n    \"\"\"\n    1. Updates variables in the Terraform variables file.\n    2. Adds variables into the main.tf template env_vars.\n    3. Adds environment variables to env_vars.json.\n\n    prevent_destroy_ecr:\n        True\n    project_name:\n        \"mlops\"\n    \"\"\"\n    setup_configurations(\n        prevent_destroy_ecr=kwargs.get('prevent_destroy_ecr'),\n        project_name=kwargs.get('project_name'),\n    )\n\n    \"\"\"\n    Use the current environment variables as the environment variables in production.\n    Change this if you want different values.\n    In a real world environment, we’d have different values but this is here for \n    demonstration purposes and for convenience.\n    \"\"\"\n    set_environment_variables(\n        password=kwargs.get('password', os.getenv('POSTGRES_PASSWORD')),\n        username=kwargs.get('username', os.getenv('POSTGRES_USER')),\n        smtp_email=kwargs.get('smtp_email', os.getenv('SMTP_EMAIL')),\n        smtp_password=kwargs.get('smtp_password', os.getenv('SMTP_PASSWORD')),\n    )",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'pink',
      configuration: {
        file_path: 'unit_5_deploying/custom/teardown_deployed_resources.py',
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['deploy'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Teardown deployed resources',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'custom',
      upstream_blocks: ['infrastructure_setup'],
      uuid: 'teardown_deployed_resources',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from mlops.utils.deploy.terraform.cli import terraform_destroy\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    if kwargs.get('destroy'):\n        terraform_destroy()\n    else:\n        print('Skipping Terraform destroy...')",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'teal',
      configuration: {
        file_path: 'unit_5_deploying/custom/deploy.py',
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['ci_and_cd'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Deploy',
      language: 'python',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'custom',
      upstream_blocks: ['teardown_deployed_resources'],
      uuid: 'deploy',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from mlops.utils.deploy.aws import update_boto3_client\nfrom mlops.utils.deploy.terraform.cli import terraform_apply\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef deploy(*args, **kwargs):\n    update_boto3_client()\n    terraform_apply()",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Version control',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'version_control_docs',
      callback_blocks: [],
      conditional_blocks: [],
      content: '## Version control\n',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.989274,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'CICD',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'cicd',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '## Continuous Integration and Continuous Deployment (CI/CD)\n\nThis allows developers to automate the process of testing and deploying code.\nIn this section, we will create a CI/CD pipeline that will build and\ndeploy our application to AWS Elastic Container Service (ECS).\n\n<br />\n\n---\n\n<br />\n\n### GitHub Actions YAML configurations\n\nIf you deployed Mage using the Terraform templates provided by Mage,\na GitHub Action workflow YAML file will already be created. \nSkip this section and go to the **Create IAM policy** section to continue.\n\nIf you don’t have the GitHub Actions workflow YAML,\nfollow this [detailed guide](https://docs.mage.ai/production/ci-cd/local-cloud/github-actions)\nto create a CI/CD pipeline for your application.\n\n> Note: After running `terraform apply`, a GitHub Actions workflow YAML file should be created\n> in the `mage-ops/.github/workflows` directory.\n> This file will contain the CI/CD pipeline configuration.\n\nEnter the following environment variables in the GitHub Actions YAML file that match your\ninfrastructure:\n\n```yaml\nenv:\n  AWS_REGION: us-west-2\n  ECR_REPOSITORY: ...\n  ECS_CLUSTER: ...\n  ECS_SERVICE: ...\n  ECS_TASK_DEFINITION: ...\n  CONTAINER_NAME: ...\n```\n\n<br />\n\n---\n\n<br />\n\n### Create IAM policy\n\n1. Policy name: **ContinuousIntegrationContinuousDeployment**\n\n1. Permissions using the JSON editor:\n\n```json\n{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Action": [\n        "ecr:BatchCheckLayerAvailability",\n        "ecr:CompleteLayerUpload",\n        "ecr:GetAuthorizationToken",\n        "ecr:InitiateLayerUpload",\n        "ecr:PutImage",\n        "ecr:UploadLayerPart",\n        "ecs:DeregisterTaskDefinition",\n        "ecs:DescribeClusters",\n        "ecs:DescribeServices",\n        "ecs:DescribeTaskDefinition",\n        "ecs:RegisterTaskDefinition",\n        "ecs:UpdateService",\n        "iam:PassRole"\n      ],\n      "Resource": "*"\n    }\n  ]\n}\n```\n\n<br />\n\n---\n\n<br />\n\n### Create IAM user\n\n1. User name: **MageContinuousIntegrationDeployer**\n\n1. Attach policies directly:\n   - **ContinuousIntegrationContinuousDeployment**\n\n1. After creating the user, create an access key and secret key.\n\n1. Use case: **Third-party service**\n',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.989418,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: 'unit_3_observability/markdowns/add_secrets_to_github_repo.md',
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Add secrets to GitHub repo',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'add_secrets_to_github_repo',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '## Add secrets to GitHub repository\n\nFollow [this guide](https://docs.mage.ai/production/ci-cd/local-cloud/github-actions#github-actions-setup) \nto add the newly created access key and secret to your GitHub project.\n\nGet the access key and secret by running on your host machine (e.g. local computer):\n\n```sh\ncat ~/.aws/credentials\n```\n\nYou should see something like this:\n\n```bash\n[MageDeployer]\naws_access_key_id = XXXXXXXXXXXXXXXXXXXX\naws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n[MageContinuousIntegrationDeployer]\naws_access_key_id = XXXXXXXXXXXXXXXXXXXX\naws_secret_access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n```',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-16 12:07:35.124087+00:00',
            description: null,
            name: 'Deploying to production',
            tags: [],
            type: 'python',
            uuid: 'deploying_to_production',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.989545,
        },
      ],
    },
    {
      all_upstream_blocks_executed: false,
      color: 'purple',
      configuration: {
        file_path: 'unit_5_deploying/custom/ci_and_cd.py',
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'CI and CD',
      language: 'python',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'custom',
      upstream_blocks: ['deploy'],
      uuid: 'ci_and_cd',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from mlops.utils.deploy.aws import (\n    IAM_USER_NAME_CICD,\n    POLICY_NAME_GITHUB_ACTIONS_DEPLOY_MAGE,\n    GITHUB_ACTIONS_DEPLOY_URL,\n    attach_policy_to_user,\n    create_access_key_for_user,\n    create_policy,\n    create_user,\n    save_credentials_to_file,\n)\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef setup(*args, **kwargs):\n    # Create IAM policy ContinuousIntegrationContinuousDeployment\n    policy_arn = create_policy(\n        POLICY_NAME_GITHUB_ACTIONS_DEPLOY_MAGE, GITHUB_ACTIONS_DEPLOY_URL\n    )\n\n    # Create the user MageContinuousIntegrationDeployer\n    create_user(IAM_USER_NAME_CICD)\n\n    # Attach policy to the user MageContinuousIntegrationDeployer\n    attach_policy_to_user(IAM_USER_NAME_CICD, policy_arn)\n\n    # Create access key\n    access_key, secret_key = create_access_key_for_user(IAM_USER_NAME_CICD)\n    save_credentials_to_file(IAM_USER_NAME_CICD, access_key, secret_key)",
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [],
    },
  ],
  callbacks: [],
  conditionals: [],
  widgets: [],
  extensions: {},
  updated_at: '2024-05-18T13:18:01.460018+00:00',
} as PipelineType;

const PIPELINE_TRAINING = {
  cache_block_output_in_memory: false,
  concurrency_config: {},
  created_at: '2024-05-15 01:11:30.960600+00:00',
  data_integration: null,
  description:
    'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
  executor_config: {},
  executor_count: 1,
  executor_type: null,
  name: 'sklearn training',
  notification_config: {},
  remote_variables_dir: null,
  retry_config: {},
  run_pipeline_in_one_process: false,
  settings: {
    triggers: null,
  },
  tags: ['business critical'],
  type: 'python',
  uuid: 'sklearn_training',
  variables_dir: '/root/.mage_data/unit_3_observability',
  variables: {
    max_evaluations: 1,
    random_state: 7,
  },
  spark_config: {},
  blocks: [
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        global_data_product: {
          outdated_after: {
            seconds: 120,
          },
          project: 'unit_3_observability',
          uuid: 'training_set',
        },
      },
      downstream_blocks: ['hyperparameter_tuning/sklearn'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Training set',
      language: 'python',
      retry_config: {},
      status: 'executed',
      timeout: null,
      type: 'global_data_product',
      upstream_blocks: [],
      uuid: 'training_set',
      callback_blocks: [],
      conditional_blocks: [],
      content: '',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 01:11:30.960600+00:00',
            description:
              'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
            name: 'sklearn training',
            tags: ['business critical'],
            type: 'python',
            uuid: 'sklearn_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.983915,
        },
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 07:16:01.736430+00:00',
            description:
              'XGBoost is a scalable and efficient implementation of gradient boosted decision trees, a powerful ensemble machine learning technique.',
            name: 'XGBoost training',
            tags: ['xgboost', 'decision tree'],
            type: 'python',
            uuid: 'xgboost_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.985061,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Dynamic block info',
      language: 'markdown',
      retry_config: null,
      status: 'updated',
      timeout: null,
      type: 'markdown',
      upstream_blocks: [],
      uuid: 'dynamic_block_info',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        '# Dynamic blocks\n\nA dynamic block will create multiple downstream blocks at runtime.\n\nThe number of blocks it creates is equal to the number of items in the output data of the dynamic block multiplied by the number of its downstream blocks.\n\nRead more about it [here](https://docs.mage.ai/design/blocks/dynamic-blocks).',
      outputs: [],
      metadata: {},
      tags: [],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 01:11:30.960600+00:00',
            description:
              'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
            name: 'sklearn training',
            tags: ['business critical'],
            type: 'python',
            uuid: 'sklearn_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.984037,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: 'teal',
      configuration: {
        dynamic: true,
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['hyperparameter_tuning/sklearn'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Load models',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'custom',
      upstream_blocks: [],
      uuid: 'load_models',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from typing import Dict, List, Tuple\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef models(*args, **kwargs) -> Tuple[List[str], List[Dict[str, str]]]:\n    \"\"\"\n    models: comma separated strings\n        linear_model.Lasso\n        linear_model.LinearRegression\n        svm.LinearSVR\n        ensemble.ExtraTreesRegressor\n        ensemble.GradientBoostingRegressor\n        ensemble.RandomForestRegressor\n    \"\"\"\n    model_names: str = kwargs.get(\n        'models', 'linear_model.LinearRegression,linear_model.Lasso'\n    )\n    child_data: List[str] = [\n        model_name.strip() for model_name in model_names.split(',')\n    ]\n    child_metadata: List[Dict] = [\n        dict(block_uuid=model_name.split('.')[-1]) for model_name in child_data\n    ]\n\n    return child_data, child_metadata\n",
      outputs: [],
      metadata: {},
      tags: ['dynamic'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 01:11:30.960600+00:00',
            description:
              'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
            name: 'sklearn training',
            tags: ['business critical'],
            type: 'python',
            uuid: 'sklearn_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.9841,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: ['sklearn'],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'Hyperparameter tuning/sklearn',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'transformer',
      upstream_blocks: ['training_set', 'load_models'],
      uuid: 'hyperparameter_tuning/sklearn',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "import sys\n\n# Add your custom directory to the Python path\nsys.path.append('/home/src/default_repo/mlops')\n\nfrom typing import Callable, Dict, Tuple, Union\n\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\n\nfrom mlops.utils.models.sklearn import load_class, tune_hyperparameters\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef hyperparameter_tuning(\n    training_set: Dict[str, Union[Series, csr_matrix]],\n    model_class_name: str,\n    *args,\n    **kwargs,\n) -> Tuple[\n    Dict[str, Union[bool, float, int, str]],\n    csr_matrix,\n    Series,\n    Callable[..., BaseEstimator],\n]:\n    print(training_set)\n    X, X_train, X_val, y, y_train, y_val, _ = training_set['build']\n\n    model_class = load_class(model_class_name)\n\n    hyperparameters = tune_hyperparameters(\n        model_class,\n        X_train=X_train,\n        y_train=y_train,\n        X_val=X_val,\n        y_val=y_val,\n        max_evaluations=kwargs.get('max_evaluations'),\n        random_state=kwargs.get('random_state'),\n    )\n\n    return hyperparameters, X, y, dict(cls=model_class, name=model_class_name)\n",
      outputs: [
        {
          sample_data: {
            columns: ['output'],
            rows: [[{}]],
          },
          shape: [1, 1],
          type: 'table',
          variable_uuid: 'output_1',
          multi_output: true,
        },
      ],
      metadata: {},
      tags: ['dynamic_child'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 01:11:30.960600+00:00',
            description:
              'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
            name: 'sklearn training',
            tags: ['business critical'],
            type: 'python',
            uuid: 'sklearn_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.984226,
        },
      ],
    },
    {
      all_upstream_blocks_executed: true,
      color: null,
      configuration: {
        file_source: {
          path: null,
        },
      },
      downstream_blocks: [],
      executor_config: null,
      executor_type: 'local_python',
      has_callback: false,
      name: 'sklearn',
      language: 'python',
      retry_config: null,
      status: 'executed',
      timeout: null,
      type: 'data_exporter',
      upstream_blocks: ['hyperparameter_tuning/sklearn'],
      uuid: 'sklearn',
      callback_blocks: [],
      conditional_blocks: [],
      content:
        "from typing import Callable, Dict, Tuple, Union\n\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\n\nfrom mlops.utils.models.sklearn import load_class, train_model\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef train(\n    settings: Tuple[\n        Dict[str, Union[bool, float, int, str]],\n        csr_matrix,\n        Series,\n        Dict[str, Union[Callable[..., BaseEstimator], str]],\n    ],\n    **kwargs,\n) -> Tuple[BaseEstimator, Dict[str, str]]:\n    hyperparameters, X, y, model_info = settings\n\n    model_class = model_info['cls']\n    model = model_class(**hyperparameters)\n    model.fit(X, y)\n\n    return model, model_info\n",
      outputs: [
        {
          sample_data: {
            columns: ['output'],
            rows: [[{}]],
          },
          shape: [1, 1],
          type: 'table',
          variable_uuid: 'output_1',
          multi_output: true,
        },
      ],
      metadata: {},
      tags: ['dynamic_child'],
      pipelines: [
        {
          added_at: null,
          pipeline: {
            created_at: '2024-05-15 01:11:30.960600+00:00',
            description:
              'Train models from the sklearn library (e.g. ExtraTreesRegressor, GradientBoostingRegressor, Lasso, LinearRegression, LinearSVR, RandomForestRegressor).',
            name: 'sklearn training',
            tags: ['business critical'],
            type: 'python',
            uuid: 'sklearn_training',
            repo_path: 'unit_3_observability',
          },
          updated_at: 1718600724.984351,
        },
      ],
    },
  ],
  callbacks: [],
  conditionals: [],
  widgets: [],
  extensions: {},
  updated_at: '2024-05-29T16:05:04.688641+00:00',
} as PipelineType;

export const PIPELINE_RAG: PipelineType = {
  type: PipelineTypeEnum.PYTHON,
  uuid: 'Mager data planeswalker',
  blocks: [
    {
      type: BlockTypeEnum.PIPELINE,
      uuid: PIPELINE_DYNAMIC.uuid,
    },
    {
      type: BlockTypeEnum.PIPELINE,
      uuid: PIPELINE_TRAINING.uuid,
    },
    {
      type: BlockTypeEnum.PIPELINE,
      uuid: PIPELINE_DEPLOY.uuid,
    },
  ],
};

PIPELINE_DYNAMIC.groups = ['data preparation'];

export default {
  PIPELINE_DYNAMIC,
  PIPELINE_DEPLOY,
  PIPELINE_TRAINING,
  PIPELINE_RAG,
};
