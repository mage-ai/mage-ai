---
title: "Overview"
---

## Introduction

Each streaming pipeline has three components:
- Source: The data stream source to consume message from.
- Transformer: Write python code to transform the message before writing to destination.
- Sink (destination): The data stream destination to write message to.

Check out this [tutorial](https://docs.mage.ai/guides/streaming-pipeline#create-a-new-pipeline) to set up an example streaming pipeline.


## Supported sources
- [Azure Event Hub](sources/azure-event-hub)
- [Kafka](sources/kafka)
- [Kinesis](sources/kinesis)
- [RabbitMQ](streaming-pipeline-rabbitmq)

## Supported sinks (destinations)

- [Amazon S3](destinations/amazon-s3)
- [Kafka](destinations/kafka)
- [Kinesis](destinations/kinesis)
- [Opensearch](destinations/opensearch)
- [Redshift (via Kinesis)](destinations/redshift)


## Test pipeline execution
After finishing configuring the streaming pipeline, you can click the button `Execution pipeline` to test streaming pipeline execution.


## Contributing guide

Follow this [doc](contributing) to add a new source or destination (sink) to Mage streaming pipeline.
