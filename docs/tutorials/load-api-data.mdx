---
title: "Load, transform, and export restaurant data"
sidebarTitle: "ETL pipeline"
---

![Restaurant](/media/restaurant-food.gif)

In this tutorial, we’ll create a data pipeline that does the following:

1. Load data from an online endpoint
2. Visualize the data using charts
3. Transform the data and create 2 new columns
4. Write the transformed data to PostgreSQL

If you prefer to skip the tutorial and view the finished code, follow
[this guide](/tutorials/etl/complete-project).

If you haven’t setup a project before, check out the
[setup guide](/getting-started/setup) before starting.

---

## 1. Create new pipeline

In the top left corner, click `File > New pipeline`. Then, click the name of the
pipeline and rename it to `etl demo`.

![Create new pipeline](/media/create-new-pipeline.gif)

---

## 2. Load data

1. Click the `+ Data loader` button, select `Python`, then click the template
   called `API`.
2. Rename the block to `load dataset`.
3. In the function named `load_data_from_api`, set the `url` variable to:
   `https://raw.githubusercontent.com/mage-ai/datasets/master/restaurant_user_transactions.csv`.
4. Run the block by clicking the play icon button or using the keyboard
   shortcuts `⌘ + Enter`, `Control + Enter`, or `Shift + Enter`.

After you run the block (⌘ + Enter), you can immediately see a sample of the
data in the block’s output.

![](/media/load-data.gif)

Here is what the code should look like:

```python
import io
import pandas as pd
import requests
from pandas import DataFrame

if 'data_loader' not in globals():
    from mage_ai.data_preparation.decorators import data_loader
if 'test' not in globals():
    from mage_ai.data_preparation.decorators import test


@data_loader
def load_data_from_api() -> DataFrame:
    """
    Template for loading data from API
    """
    url = 'https://raw.githubusercontent.com/mage-ai/datasets/master/restaurant_user_transactions.csv'

    response = requests.get(url)
    return pd.read_csv(io.StringIO(response.text), sep=',')


@test
def test_output(df) -> None:
    """
    Template code for testing the output of the block.
    """
    assert df is not None, 'The output is undefined'
```

---

## 3. Visualize data

### 3a. Distribution of ratings

We’ll add a chart to visualize how frequent people give 1 star, 2 star, 3 star,
4 star, or 5 star ratings.

1. Click the `+ Add chart` button in the top right corner, then click
   `Histogram`.
2. Click the pencil icon in the top right corner of the chart to edit the chart.
3. In the dropdown labeled "Number column for chart", select the column
   `rating`.
4. Click the play button icon in the top right corner of the chart to run the
   chart.

![histogram demo](/media/histogram.gif)

Your chart should look like:

![histogram chart](/media/histogram.jpeg)

### 3b. Number of meals per user

Let’s add another chart to see how many meals each user has.

1. Click the `+ Add chart` button in the top right corner, then click
   `Bar chart`.
2. Click the pencil icon in the top right corner of the chart to edit the chart.
3. In the dropdown labeled "Group by columns", select the column `user ID`.
4. Under the "Metrics" section, in the dropdown labeled "aggregation", select
   `count_distinct`.
5. Under the "Metrics" section, in the dropdown labeled "column", select
   `meal transaction ID`.
6. Click the play button icon in the top right corner of the chart to run the
   chart.

![bar-chart demo](/media/bar-chart.gif)

Your chart should look like:

![bar-chart chart](/media/bar-chart.jpeg)

## 4. Transform data

Let’s transform the data in 2 ways:
- Add a column that counts the number of meals for each user.
- Clean the column names to properly store in a PostgreSQL database.

<br />

1. Click the `+ Transformer` button, select `Python`, select `Generic`.
1. Rename the block to `transform data`.
1. Paste the following code into the block:

    ```python
    def number_of_rows_per_key(df, key, column_name):
        data = df.groupby(key)[key].agg(['count'])
        data.columns = [column_name]
        return data


    def clean_column(column_name):
        return column_name.lower().replace(' ', '_')


    @transformer
    def transform(df, *args, **kwargs):
        # Add number of meals for each user
        df_new_column = number_of_rows_per_key(df, 'user ID', 'number of meals')
        df = df.join(df_new_column, on='user ID')

        # Clean column names
        df.columns = [clean_column(col) for col in df.columns]

        return df


    @test
    def test_output(output, *args) -> None:
        """
        Template code for testing the output of the block.
        """
        assert output is not None, 'The output is undefined'
    ```

1. Run the block by clicking the play icon button or using the keyboard
   shortcuts `⌘ + Enter`, `Control + Enter`, or `Shift + Enter`.

![transform](/media/transform.gif)

---

## 5. Export data to PostgreSQL

### 5a. Add PostgreSQL credentials in `io_config.yaml` file.

1. On the left side of the screen in the file browser, click on the file named
   `io_config.yaml`.
2. Then, paste the following credentials:

```yaml
version: 0.1.1
default:
  POSTGRES_DBNAME: mage/demo2
  POSTGRES_USER: mage
  POSTGRES_PASSWORD: v2_3upzD_eMSdiu5AMjgzSbi3K7KTAuE
  POSTGRES_HOST: db.bit.io
  POSTGRES_PORT: 5432
```

3. Save the file by pressing `⌘ + Enter` or `Control + Enter`.
4. Close the file by pressing the `X` button on the right of the file name at
   the top of the screen.

![credentials](/media/credentials.gif)

### 5b. Export data

1. Click the `+ Data exporter` button and select `SQL`.
2. Under the `Data provider` dropdown, select `Postgres`.
3. Under the `Profile` dropdown, select `default`.
4. In the input field labeled `Save to schema:`, enter `mage`.
5. Under the `Write policy` dropdown, select `Replace`.
6. Enter the following SQL query in the code block: `SELECT * FROM {{ df_1 }}`
7. Execute the entire pipeline by pressing the `Execute pipeline` button on in
   your sidekick on the right.

![Data exporter SQL](https://github.com/mage-ai/assets/blob/main/pipelines/data-exporter-sql.gif?raw=true)

Your output should look something like this:

```
aving current pipeline config for backup. This may take some time...
[load_dataset] Executing data_loader block...
[load_dataset] --------------------------------------------------------------
[load_dataset] 1/1 tests passed.
[load_dataset] DONE
[transform_data] Executing transformer block...
[transform_data] --------------------------------------------------------------
[transform_data] 1/1 tests passed.
[transform_data] DONE
[morning_sun] Executing data_exporter block...
[morning_sun] Postgres initialized
[morning_sun] └─ Opening connection to PostgreSQL database...
[morning_sun] DONE
[morning_sun]
[morning_sun] ├─
[morning_sun] └─ Exporting data to 'mage.dev_etl_demo_morning_sun_v1'...
[morning_sun] DONE
[morning_sun]
[morning_sun] ├─
[morning_sun] └─ Loading data...
[morning_sun] DONE
[morning_sun] --------------------------------------------------------------
[morning_sun] 0/0 tests passed.
[morning_sun] DONE
Pipeline etl_demo execution complete.
You can see the code block output in the corresponding code block.
```

---

## Congratulations!

You’ve successfully built a pipeline for loading data, transforming it, and
exporting it to PostgreSQL.

If you have more questions or ideas, please live chat with us in
[Slack](https://www.mage.ai/chat)
