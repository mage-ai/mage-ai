---
title: "Overview"
---

## Introduction

Each streaming pipeline has three components:
- Source: The data stream source to consume message from.
- Transformer: Write python code to transform the message before writing to destination.
- Sink (destination): The data stream destination to write message to.

Check out this [tutorial](/guides/streaming-pipeline#create-a-new-pipeline) to set up an example streaming pipeline.

<Note>In version `0.8.80` or greater.</Note>

You can build a streaming pipeline with one source, multiple transformers, and multiple sinks. Here is an example streaming
pipeline:
![Complex streaming pipeline](https://media.graphassets.com/4ZWHyLpgTESu0rB1sXnP)

## Supported sources
- [Amazon SQS](/guides/streaming/sources/amazon-sqs)
- [Azure Event Hub](/guides/streaming/sources/azure-event-hub)
- [Kafka](/guides/streaming/sources/kafka)
- [Kinesis](/guides/streaming/sources/kinesis)
- [RabbitMQ](/guides/streaming/streaming-pipeline-rabbitmq)

## Supported sinks (destinations)

- [Amazon S3](/guides/streaming/destinations/amazon-s3)
- [Dummy](/guides/streaming/destinations/dummy)
- [Kafka](/guides/streaming/destinations/kafka)
- [Kinesis](/guides/streaming/destinations/kinesis)
- [MongoDB](/guides/streaming/destinations/mongodb)
- [Opensearch](/guides/streaming/destinations/opensearch)
- [Redshift (via Kinesis)](/guides/streaming/destinations/redshift)


## Test pipeline execution
After finishing configuring the streaming pipeline, you can click the button `Execution pipeline` to test streaming pipeline execution.

## Run pipeline in production
Create the trigger in [triggers page](/design/data-pipeline-management#create-trigger) to run streaming pipelines in production.

### Executor count
If you want to run multiple executors at the same time to scale the streaming pipeline execution, you can set the `executor_count` variable
in the pipeline's metadata.yaml file. Here is an example:
```yaml
blocks:
- ...
- ...
executor_count: 10
name: test_streaming_pipeline
type: streaming
uuid: test_streaming_pipeline
```

### Executor type
When running Mage on Kubernetes cluster, you can also configure streaming pipeline to be run on separate k8s pods by setting `executor_type`
field in the pipeline's metadata.yaml to `k8s`.

Example config:
```yaml
blocks:
- ...
- ...
executor_type: k8s
name: test_streaming_pipeline
type: streaming
uuid: test_streaming_pipeline
```

## Contributing guide

Follow this [doc](contributing) to add a new source or destination (sink) to Mage streaming pipeline.
